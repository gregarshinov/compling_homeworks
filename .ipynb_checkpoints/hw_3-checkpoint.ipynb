{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа 3\n",
    "## Аршинов Г.Е."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize\n",
    "from string import punctuation\n",
    "from itertools import islice\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deletions(word: str) -> List[str]:\n",
    "    result = []\n",
    "    for i, char in enumerate(word):\n",
    "        result.append(''.join(ch for ch_idx, ch in enumerate(word) if ch_idx != i))\n",
    "    return tuple(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ello', 'hllo', 'helo', 'helo', 'hell')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_deletions(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"../compling_nlp_hse_course/data/corpus_500.txt\", 'r') as f:\n",
    "    corpus_raw = f.read()\n",
    "with open('../compling_nlp_hse_course/data/lenta-ru-news.csv', 'r') as archive:\n",
    "    reader = csv.reader(archive, delimiter=',', quotechar='\"')\n",
    "    for i, line in islice(enumerate(reader), 5000):\n",
    "        corpus_raw += (line[2].replace('\\xa0', ' ') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "tokenizer = regex.compile(r\"(?:\\w+-)?\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "ru_stopwords = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [word.lower() for word in tokenizer.findall(corpus_raw) if word.lower() not in ru_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_corpus = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in counter:\n",
    "    typo_variants = generate_deletions(word)\n",
    "    for variant in typo_variants:\n",
    "        if variant not in ready_corpus:\n",
    "            ready_corpus[variant] = [word]\n",
    "        else:\n",
    "            ready_corpus[variant].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_correction(word):\n",
    "    if word in counter:\n",
    "        return word\n",
    "    if word in ready_corpus:\n",
    "        return sorted([(variant, counter[variant]) for variant in ready_corpus[word]], key=lambda x: x[1], reverse=True)[0][0]\n",
    "    typo_variants = generate_deletions(word)\n",
    "    result = []\n",
    "    for variant in typo_variants:\n",
    "        suggestions = ready_corpus.get(variant)\n",
    "        if suggestions:\n",
    "            result.extend(suggestions)\n",
    "    top_variants = sorted([(variant, counter[variant]) for variant in result], key=lambda x: x[1], reverse=True)\n",
    "    return next(iter(top_variants), word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('../compling_nlp_hse_course/data/sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('../compling_nlp_hse_course/data/correct_sents.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
    "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        predicted = cashed.get(pair[1], suggest_correction(pair[1]))\n",
    "        cashed[pair[0]] = predicted\n",
    "        if predicted == pair[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] !=  predicted:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == predicted:\n",
    "                mistaken_fixed += 1\n",
    "    if not i % 100:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47332667332667333\n",
      "0.17114351496546432\n",
      "0.4814517055242908\n"
     ]
    }
   ],
   "source": [
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_corrections(word):\n",
    "    if word in counter:\n",
    "        return [(word, 1)]\n",
    "    if word in ready_corpus:\n",
    "        return sorted([(variant, counter[variant]) for variant in ready_corpus[word]], key=lambda x: x[1], reverse=True)\n",
    "    typo_variants = generate_deletions(word)\n",
    "    result = []\n",
    "    for variant in typo_variants:\n",
    "        suggestions = ready_corpus.get(variant)\n",
    "        if suggestions:\n",
    "            result.extend(suggestions)\n",
    "    return sorted([(variant, counter[variant]) for variant in result], key=lambda x: x[1], reverse=True) or [(word, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    return [word.strip(punctuation) for word in text.lower().split() if word]\n",
    "\n",
    "def window(collection, size):\n",
    "    for i in range(len(collection) - size + 1):\n",
    "        yield tuple(collection[inc] for inc in range(i, i + size))\n",
    "\n",
    "def pad(collection, left=\"<SOS>\", right=\"<EOS>\"):\n",
    "    return [left] + collection + [right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [pad(pad(normalize(text))) for text in sent_tokenize(corpus_raw)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = Counter()\n",
    "bigrams = Counter()\n",
    "trigrams = Counter()\n",
    "\n",
    "for sentence in sentences:\n",
    "    unigrams.update(tuple(sentence))\n",
    "    bigrams.update(tuple(window(sentence, 2)))\n",
    "    trigrams.update(tuple(window(sentence, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = lil_matrix((len(bigrams), len(unigrams)))\n",
    "\n",
    "id2bigram = list(bigrams)\n",
    "bigram2id = {bigram:i for i, bigram in enumerate(id2bigram)}\n",
    "id2word = list(unigrams)\n",
    "word2id = {word:i for i, word in enumerate(id2word)}\n",
    "\n",
    "for word_1, word_2, word_3 in trigrams:\n",
    "    bigram = (word_1, word_2)\n",
    "    ngram = (word_1, word_2, word_3)\n",
    "    matrix[bigram2id[bigram], word2id[word_3]] = (trigrams[ngram] / bigrams[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.7333333333333333\n",
      "50\n",
      "0.5797101449275363\n",
      "100\n",
      "0.5842293906810035\n",
      "150\n",
      "0.5861850443599493\n",
      "200\n",
      "0.5868205868205868\n",
      "250\n",
      "0.5927286081527727\n",
      "300\n",
      "0.5886418269230769\n",
      "350\n",
      "0.5906830672598796\n",
      "400\n",
      "0.5931338436557643\n",
      "450\n",
      "0.5914583333333333\n",
      "500\n",
      "0.5932392273402675\n",
      "550\n",
      "0.5926680244399185\n",
      "600\n",
      "0.5924441017733231\n",
      "650\n",
      "0.5897580987409817\n",
      "700\n",
      "0.591732283464567\n",
      "750\n",
      "0.5917464996315401\n",
      "800\n",
      "0.5887850467289719\n",
      "850\n",
      "0.5881529348411416\n",
      "900\n",
      "0.586024781637213\n"
     ]
    }
   ],
   "source": [
    "mistakes = []\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "\n",
    "\n",
    "for i, true_sentence in enumerate(true):\n",
    "    bad_sentence = bad[i]\n",
    "    word_pairs = align_words(true_sentence, bad_sentence)\n",
    "    \n",
    "    word_pairs = [(\"<SOS>\", \"<SOS>\")] + word_pairs\n",
    "    pred_sent = []\n",
    "    for pair_idx, pair in islice(enumerate(word_pairs), 1, None):\n",
    "        word, bad_word = pair\n",
    "        pred = None\n",
    "        predicted = suggest_corrections(word)\n",
    "        \n",
    "        \n",
    "        prev_word = word_pairs[word_idx-1][1]\n",
    "        \n",
    "        if prev_word not in unigrams:\n",
    "            pred = predicted[0][0]\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            lm_predicted = []\n",
    "            for variant, freq in predicted:\n",
    "                bigram = (prev_word, variant)\n",
    "                try:\n",
    "                    prob = matrix[bigram2id[bigram], word2id[variant]]\n",
    "                except KeyError:\n",
    "                    prob = 0\n",
    "                lm_predicted.append((variant, prob))\n",
    "            if lm_predicted:\n",
    "                pred = sorted(lm_predicted, key=lambda x: -x[1])[0][0]\n",
    "            \n",
    "        \n",
    "        if pred is None:\n",
    "            pred = bad_word\n",
    "        if pred == word:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes.append((word, bad_word, pred))\n",
    "        total += 1\n",
    "            \n",
    "        if word == bad_word:\n",
    "            total_correct += 1\n",
    "            if word !=  pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if word == pred:\n",
    "                mistaken_fixed += 1\n",
    "    \n",
    "    if not i % 50:\n",
    "        print(i)\n",
    "        print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5863136863136863\n",
      "0.8119723714504988\n",
      "0.4474560698288733\n"
     ]
    }
   ],
   "source": [
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

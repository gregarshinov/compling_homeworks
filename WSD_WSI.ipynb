{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation (снятие лексической неоднозначности) и Word Sense Induction (нахождение значений слова)\n",
    "## Arshinov Girgory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adagram\n",
    "from lxml import html\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from string import punctuation\n",
    "import json, os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "class FasterMorphology:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__morph = MorphAnalyzer()\n",
    "        self.__cache = {}\n",
    "    \n",
    "    def parse(self, word: str):\n",
    "        if word in self.__cache:\n",
    "            return self.__cache[word]\n",
    "        else:\n",
    "            analysis = self.__morph.parse(word)\n",
    "            self.__cache[word] = analysis[0]\n",
    "            return analysis[0]\n",
    "\n",
    "\n",
    "morph = FasterMorphology()\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split() if word and word not in stops]\n",
    "    words = [word for word in words if word]\n",
    "\n",
    "    return words\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = tokenize(text)\n",
    "    words = [morph.parse(word).normal_form for word in words if word]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель обученная на большом корпусе (острожно 1.5 гб)\n",
    "# !curl 'https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib' --output all.a010.p10.d300.w5.m100.nonorm.slim.joblib\n",
    "vm = adagram.VectorModel.load('all.a010.p10.d300.w5.m100.nonorm.slim.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание № 1. Протестировать адаграм в определении перефразирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('../compling_nlp_hse_course/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT.joinpath('paraphraser/paraphrases.xml').exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(DATA_ROOT.joinpath('paraphraser/paraphrases.xml').open('rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизуйте пары текстов с помощью Адаграма, обучите любую модель и оцените качество (кросс-валидацией). \n",
    "\n",
    "За основу возьмите код из предыдущего семинара/домашки, только в функции get_embedding вам нужно выбирать вектор нужного значения (импользуйте model.disambiguate и model.sense_vector). Отдельные векторы усредните как и в предыдущем семинаре."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вытаскивания пар (целевое слово, контекстые слова) вам нужно будет написать специальную функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяте на списке из чисел, чтобы было удобно дебажить\n",
    "words = list(range(0, 10))\n",
    "\n",
    "def get_words_in_context(words, window=3):\n",
    "    words_in_context = []\n",
    "    for idx, word in enumerate(words):\n",
    "        context = []\n",
    "        for ctx_idx, ctx_word in enumerate(words):\n",
    "            residual = abs(idx - ctx_idx)\n",
    "            if residual > 0 and residual <= window:\n",
    "                context.append(ctx_word)\n",
    "        words_in_context.append([word, context])\n",
    "    return words_in_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_output = [[0, [1, 2, 3]],\n",
    " [1, [0, 2, 3, 4]],\n",
    " [2, [0, 1, 3, 4, 5]],\n",
    " [3, [0, 1, 2, 4, 5, 6]],\n",
    " [4, [1, 2, 3, 5, 6, 7]],\n",
    " [5, [2, 3, 4, 6, 7, 8]],\n",
    " [6, [3, 4, 5, 7, 8, 9]],\n",
    " [7, [4, 5, 6, 8, 9]],\n",
    " [8, [5, 6, 7, 9]],\n",
    " [9, [6, 7, 8]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_words_in_context(words) == test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда получиться такой же результат, добавьте эту функцию в get_embedding. Проходите циклом по результату работы get_words_in_context и поставляйте каждый элемент-список в model.disambiguate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_embedding_adagram(text: List[str], model: adagram.model.VectorModel, window: int = 3):\n",
    "    \n",
    "    word2context = get_words_in_context(text, window)\n",
    "    \n",
    "    \n",
    "    vectors = np.zeros((len(word2context), model.dim))\n",
    "    \n",
    "    for i, (word, context) in enumerate(word2context):\n",
    "        \n",
    "        try:\n",
    "            sense_id = model.disambiguate(word, context).argmax()\n",
    "            vectors[i] = model.sense_vector(word, sense_id)\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = vm.dim\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding_adagram(text, vm)\n",
    "\n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding_adagram(text, vm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_w2v = np.concatenate([X_text_1_w2v, X_text_2_w2v], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.55      0.51      0.53       629\n",
      "           0       0.47      0.53      0.50       737\n",
      "           1       0.36      0.33      0.35       441\n",
      "\n",
      "    accuracy                           0.47      1807\n",
      "   macro avg       0.46      0.46      0.46      1807\n",
      "weighted avg       0.47      0.47      0.47      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = data['label'].values\n",
    "print(y.shape)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_text_w2v, y,random_state=1)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
    "                             class_weight='balanced')\n",
    "clf.fit(train_X, train_y)\n",
    "preds = clf.predict(valid_X)\n",
    "print(classification_report(valid_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.142710</td>\n",
       "      <td>0.037201</td>\n",
       "      <td>0.415053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.178780</td>\n",
       "      <td>0.032576</td>\n",
       "      <td>0.471500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.280135</td>\n",
       "      <td>0.029135</td>\n",
       "      <td>0.412286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.254598</td>\n",
       "      <td>0.028071</td>\n",
       "      <td>0.394795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score\n",
       "0  5.142710    0.037201    0.415053\n",
       "1  5.178780    0.032576    0.471500\n",
       "2  5.280135    0.029135    0.412286\n",
       "3  5.254598    0.028071    0.394795"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cross_validate(clf, X_text_w2v, y, cv=4, n_jobs=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводить значения просто из текста тяжело, поэтому можно попробовать воспользоваться словарями (wsi сделанное людьми). Для русского придется парсить сайты словарей, а для английского можно воспользоваться WordNet'ом (https://wordnet.princeton.edu/), который есть в nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet - это лексическая база данных, где существительные, прилагательные и глаголы английского сгруппированы по значению. К тому же между ними установлены связи (гипонимия, гипоронимия, миронимия и т.п.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/grigoriyarshinov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# запустите если не установлен ворднет\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И даже примеры:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Реализовать алгоритм Леска и проверить его на реальном датасете"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ворднет можно использовать для дизамбигуации. Самый простой алгоритм дизамбигуации - алгоритм Леска. В нём нужное значение слова находится через пересечение слов контекста, в котором употреблено это слово, с определениями значений слова из ворднета. Значение с максимальным пересечением - нужное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word, sentence):\n",
    "    \"\"\"Ваш код тут\"\"\"\n",
    "    sentence = set(sentence)\n",
    "    bestsense = 0\n",
    "    maxoverlap = 0\n",
    "    synsets = wn.synsets(word)\n",
    "    for i, syns in enumerate(synsets):\n",
    "        definition = set(syns.definition().lower().split())\n",
    "        overlap = len(sentence & definition)\n",
    "        if overlap > maxoverlap:\n",
    "            maxoverlap = overlap\n",
    "            bestsense = i\n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk('day', 'some point or period in time'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работать функция должна как-то так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# на вход подается элемент результата работы уже написанной вами функции get_words_in_context\n",
    "lesk('day', 'some point or period in time'.split()) # для примера контекст совпадает с одним из определений\n",
    "# а на выходе индекс подходящего синсета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверьте насколько хорошо работает такой метод на реальном датасете.** http://lcl.uniroma1.it/wsdeval/ - большой фреймворк для оценки WSD. Там много данных и я взял кусочек, чтобы не было проблем с памятью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL.data.xml     ALL.gold.key.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/WSD_Unified_Evaluation_Datasets/ALL/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/WSD_Unified_Evaluation_Datasets/ALL/ALL.data.xml') as file:\n",
    "    soup = BeautifulSoup(file.read(), features='lxml')\n",
    "with open('data/WSD_Unified_Evaluation_Datasets/ALL/ALL.gold.key.txt') as file:\n",
    "    sense_keys = {line.split()[0]: line.split()[1] for line in file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import NavigableString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "not_navigable_string = lambda x: not isinstance(x, NavigableString)\n",
    "\n",
    "for text in filter(not_navigable_string, soup.corpus):\n",
    "    text_sentences = []\n",
    "    for sentence in filter(not_navigable_string, text):\n",
    "        sentence_words = []\n",
    "        for word in filter(not_navigable_string, sentence):\n",
    "            sentence_words.append({'word': word.text.strip(),\n",
    "                             'sense_id': word.get('id'),\n",
    "                             'lemma': word.get('lemma')})\n",
    "        text_sentences.append(sentence_words)\n",
    "    texts.append(text_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корпус состоит из предложений, где у каждого слова три поля - значение, лемма и само слово. Значение пустое, когда слово однозначное, а у многозначных слов стоит тэг вида **'long%3:00:02::'** Это тэг wordnet'ного формата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3080495356037152\n"
     ]
    }
   ],
   "source": [
    "match_count = 0\n",
    "word_count = 0\n",
    "for text in texts[0:1]:\n",
    "    for sentence in text:\n",
    "        sentence_tokens = [word[\"word\"] for word in sentence]\n",
    "        for idx, word in enumerate(sentence):\n",
    "            sense_id = word.get(\"sense_id\")\n",
    "            context = copy(sentence_tokens)\n",
    "            del context[idx]\n",
    "            if sense_id:\n",
    "                word_count += 1\n",
    "                actual_synset = wn.lemma_from_key(sense_keys[sense_id]).synset()\n",
    "                predicted_synset = wn.synsets(word['lemma'])[lesk(word['lemma'], context)]\n",
    "                if predicted_synset == actual_synset:\n",
    "                    match_count += 1\n",
    "\n",
    "print(match_count / word_count)\n",
    "        \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вам нужно для каждого многозначного слова (т.е. у него есть тэг в первом поле) с помощью алгоритма Леска предсказать нужный синсет и сравнить с правильным. Посчитайте процент правильных предсказаний (accuracy).**\n",
    "\n",
    "Если считается слишком долго, возьмите поменьше предложений (например, только тысячу)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительный балл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если хотите заработать дополнительный балл, попробуйте улучшить алгоритм Леска любым способом (например, использовать расстояние редактирования вместо пересечения или даже вставить машинное обучение)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
